{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from pydub import AudioSegment\n",
    "from pydub.playback import play\n",
    "import os\n",
    "from pydub import AudioSegment\n",
    "from dotenv import load_dotenv\n",
    "from transformers import pipeline, AutoProcessor, AutoModelForSpeechSeq2Seq\n",
    "import soundfile as sf\n",
    "from openai import OpenAI\n",
    "import jiwer\n",
    "import string\n",
    "import re\n",
    "from bert_score import BERTScorer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Manual Transcription Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_df = pd.read_csv(\"<path to csv file>\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you type:\n",
    "- REDO it will run that same clip again\n",
    "- EXIT will terminate the code but you can always resume from where you left off."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "files=os.listdir(\"<path to audio clips>\")\n",
    "files_sorted = sorted(files, key=lambda x: os.path.getctime(os.path.join(\"<path to audio clips\", x)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i=0\n",
    "for clip in files_sorted:\n",
    "    print(clip)\n",
    "    if(labels_df.loc[i,\"manual_transcription_done\"]):\n",
    "        i+=1\n",
    "        continue\n",
    "    play(AudioSegment.from_file(\"<path to audio clips\"+clip))\n",
    "    ground_truth = input('Enter transcript: ')\n",
    "    if(ground_truth==\"REDO\"):\n",
    "        i-=1\n",
    "    elif(ground_truth==\"EXIT\"):\n",
    "        break\n",
    "    else:\n",
    "        labels_df.loc[i,\"manual_annotation\"]=ground_truth\n",
    "        labels_df.loc[i,\"manual_transcription_done\"]=True\n",
    "        labels_df.to_csv(\"<path to csv>\",index=False)\n",
    "        foundloc='./clips_done.txt'\n",
    "        with open(foundloc, 'a') as fp:\n",
    "            nid=str(labels_df.loc[i,\"absolute_index\"])+\"\\n\"\n",
    "            fp.write(nid)\n",
    "    i+=1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Whisper2 Inference with API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def manual_get_audio_clip(show,episode_id,clip_id):\n",
    "    audio_path = \"<path to clips>\"+str(show)+\"/\"+str(episode_id)+\"/\"+str(show)+\"_\"+str(episode_id)+\"_\"+str(clip_id)+\".wav\"\n",
    "    return audio_path\n",
    "\n",
    "\n",
    "def auto_get_audio_clip(absolute_id):\n",
    "    show=labels_df[\"Show\"][labels_df[\"absolute_index\"]==absolute_id].values[0]\n",
    "    episode_id=labels_df[\"EpId\"][labels_df[\"absolute_index\"]==absolute_id].values[0]\n",
    "    clip_id=labels_df[\"ClipId\"][labels_df[\"absolute_index\"]==absolute_id].values[0]\n",
    "    return manual_get_audio_clip(show,episode_id,clip_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_file = open(auto_get_audio_clip(24106), \"rb\")\n",
    "transcript = client.audio.transcriptions.create(\n",
    "  model=\"whisper-1\",\n",
    "  file=audio_file\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(labels_df.shape[0]):\n",
    "  if(type(labels_df.loc[i, 'annotated_groundTruth'])!=str):\n",
    "    continue\n",
    "  if(labels_df.loc[i, 'Whisper2_done']==1):\n",
    "    continue\n",
    "  print(i,\" : \",labels_df.loc[i, 'absolute_index'])\n",
    "  try:\n",
    "    audio_file = open(auto_get_audio_clip(labels_df.loc[i, 'absolute_index']), \"rb\")\n",
    "    transcript = client.audio.transcriptions.create(\n",
    "      model=\"whisper-1\",\n",
    "      file=audio_file\n",
    "    )\n",
    "  except:\n",
    "    continue\n",
    "  result=transcript.text\n",
    "  labels_df.loc[i, 'Whisper2Annotation']=result.strip()\n",
    "  labels_df.loc[i, 'Whisper2_done']=1\n",
    "  labels_df.to_csv(\"<path to csv>\",index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Whisper 3 inference with Huggingface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = pipeline(\"automatic-speech-recognition\", model=\"openai/whisper-large-v3\")\n",
    "processor = AutoProcessor.from_pretrained(\"openai/whisper-large-v3\")\n",
    "model = AutoModelForSpeechSeq2Seq.from_pretrained(\"openai/whisper-large-v3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(labels_df.shape[0]):\n",
    "  if(type(labels_df.loc[i, 'annotated_groundTruth'])!=str):\n",
    "    continue\n",
    "  if(labels_df.loc[i, 'Whisper3Annotation']==1):\n",
    "    continue\n",
    "  print(i,\" : \",labels_df.loc[i, 'absolute_index'])\n",
    "  try:\n",
    "    file_path=auto_get_audio_clip(labels_df.loc[i, 'absolute_index'])\n",
    "    waveform, sample_rate = sf.read(file_path)\n",
    "    if sample_rate != 16000:\n",
    "      print(labels_df.loc[i, 'absolute_index'])\n",
    "      waveform = librosa.resample(waveform, orig_sr=sample_rate, target_sr=16000)\n",
    "      sample_rate = 16000\n",
    "    inputs = processor(waveform, sampling_rate=sample_rate, return_tensors=\"pt\")\n",
    "    input_features = inputs.input_features\n",
    "    generated_ids = model.generate(inputs=input_features, language=\"en\")\n",
    "    transcription = processor.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
    "  except:\n",
    "    continue\n",
    "  result=transcription\n",
    "  labels_df.loc[i, 'Whisper3Annotation']=result.strip()\n",
    "  labels_df.loc[i, 'whisper3_done']=1\n",
    "  labels_df.to_csv(\"<csv file path>\",index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Whisper 2 Metric Calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "whisper2_metric_df=pd.read_csv(\"<path to label_df csv file>\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_punctuation(text):\n",
    "    all_punctuation = string.punctuation\n",
    "\n",
    "    return text.translate(str.maketrans('', '', all_punctuation))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_interjection(text):\n",
    "    text_list=text.split()\n",
    "    text_size=len(text_list)\n",
    "    i=0\n",
    "    while i < text_size:\n",
    "        text_size=len(text_list)\n",
    "        if(text_size==0):\n",
    "            return \" \"\n",
    "        if(\"/i\" in text_list[i]):\n",
    "            if(\"uh/i\" in text_list[i] or \"um/i\" in text_list[i] or \"like/i\" in text_list[i] or \"mhm/i\" in text_list[i] or \"eh/i\" in text_list[i] or \"mm\" in text_list[i]):\n",
    "                text_list=text_list[:i]+text_list[i+1:]     \n",
    "                i=0\n",
    "                continue\n",
    "            elif((text_list[i-1]+text_list[i])==\"youknow/i\"):\n",
    "                text_list=text_list[:i-1]+text_list[i+1:]     \n",
    "                i=0\n",
    "                continue\n",
    "            else:\n",
    "                print(\"standoutinterject: \",text_list)\n",
    "        i+=1\n",
    "    return \" \".join(text_list)  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_ground_truth(text, method='literal'):\n",
    "  symbols = ['\\[.*?\\]', '/b', '/p', '/r', '/i', '\\<.*?\\>']\n",
    "\n",
    "  if method == 'semantic':\n",
    "\n",
    "    text = re.sub(symbols[0], '', text)\n",
    "\n",
    "    text = remove_interjection(text)\n",
    "\n",
    "  text = re.sub(symbols[-1], '', text)\n",
    "  text = text.replace('[', ' ')\n",
    "  text = text.replace(']', '')\n",
    "  for symbol in symbols[1:]:\n",
    "      text = text.replace(symbol, '')\n",
    "\n",
    "  return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for i in range(whisper2_metric_df.shape[0]):\n",
    "  if(type(whisper2_metric_df.loc[i, 'annotated_groundTruth'])!=str):\n",
    "    continue\n",
    "  print(i)\n",
    "\n",
    "  #ground truth pre-pre-processing\n",
    "  ground_truth=whisper2_metric_df.loc[i, 'annotated_groundTruth']\n",
    "  ground_truth=ground_truth.replace('<', '')\n",
    "  ground_truth=ground_truth.replace('>', '')\n",
    "  ground_truth=ground_truth.lower()\n",
    "\n",
    "  #whisper pre-processing\n",
    "  whisper_guess=whisper2_metric_df.loc[i, 'bigWhisperAnnotation']\n",
    "  if(type(whisper_guess)==float):\n",
    "    whisper_guess=\" \"\n",
    "  whisper_guess=remove_punctuation(whisper_guess)\n",
    "  whisper_guess=whisper_guess.lower()\n",
    "\n",
    "  #LITERAL STUFF\n",
    "  literal_string=remove_punctuation(clean_ground_truth(ground_truth,method=\"literal\"))\n",
    "  \n",
    "  if(literal_string.strip()==\"\" and whisper_guess.strip()==\"\"):\n",
    "    whisper2_metric_df.loc[i, 'literal_WER']=0.0\n",
    "    whisper2_metric_df.loc[i, 'literal_ref']=0\n",
    "    whisper2_metric_df.loc[i, 'literal_sub']=0.0\n",
    "    whisper2_metric_df.loc[i, 'literal_ins']=0.0\n",
    "    whisper2_metric_df.loc[i, 'literal_del']=0.0\n",
    "    \n",
    "  elif(literal_string.strip()==\"\"):\n",
    "    whisper2_metric_df.loc[i, 'literal_WER']=1.0\n",
    "    whisper2_metric_df.loc[i, 'literal_ref']=len(whisper_guess.split())\n",
    "    whisper2_metric_df.loc[i, 'literal_sub']=1.0\n",
    "    whisper2_metric_df.loc[i, 'literal_ins']=1.0\n",
    "    whisper2_metric_df.loc[i, 'literal_del']=1.0\n",
    "  else:\n",
    "    literal_results=jiwer.process_words(literal_string,whisper_guess)\n",
    "    whisper2_metric_df.loc[i, 'literal_WER']=min(1,literal_results.wer)\n",
    "    whisper2_metric_df.loc[i, 'literal_ref']=len(literal_string.split())\n",
    "    whisper2_metric_df.loc[i, 'literal_sub']=literal_results.substitutions\n",
    "    whisper2_metric_df.loc[i, 'literal_ins']=literal_results.insertions\n",
    "    whisper2_metric_df.loc[i, 'literal_del']=literal_results.deletions\n",
    "\n",
    "\n",
    "  #SEMANTIC STUFF\n",
    "  semantic_string=remove_punctuation(clean_ground_truth(ground_truth,method=\"semantic\"))\n",
    "\n",
    "  if(semantic_string.strip()==\"\" and whisper_guess.strip()==\"\"):\n",
    "    whisper2_metric_df.loc[i, 'semantic_WER']=0.0\n",
    "    whisper2_metric_df.loc[i, 'semantic_ref']=0.0\n",
    "    whisper2_metric_df.loc[i, 'semantic_sub']=0.0\n",
    "    whisper2_metric_df.loc[i, 'semantic_ins']=0.0\n",
    "    whisper2_metric_df.loc[i, 'semantic_del']=0.0\n",
    "    \n",
    "  elif(semantic_string.strip()==\"\"):\n",
    "    whisper2_metric_df.loc[i, 'semantic_WER']=1.0\n",
    "    whisper2_metric_df.loc[i, 'semantic_ref']=len(whisper_guess.split())\n",
    "    whisper2_metric_df.loc[i, 'semantic_sub']=1.0\n",
    "    whisper2_metric_df.loc[i, 'semantic_ins']=1.0\n",
    "    whisper2_metric_df.loc[i, 'semantic_del']=1.0\n",
    "  else:\n",
    "    semantic_results=jiwer.process_words(semantic_string,whisper_guess)\n",
    "    whisper2_metric_df.loc[i, 'semantic_WER']=min(1,semantic_results.wer)\n",
    "    whisper2_metric_df.loc[i, 'semantic_ref']=len(semantic_string.split())\n",
    "    whisper2_metric_df.loc[i, 'semantic_sub']=semantic_results.substitutions\n",
    "    whisper2_metric_df.loc[i, 'semantic_ins']=semantic_results.insertions\n",
    "    whisper2_metric_df.loc[i, 'semantic_del']=semantic_results.deletions\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for i in range(whisper2_metric_df.shape[0]):\n",
    "  if(type(whisper2_metric_df.loc[i, 'annotated_groundTruth'])!=str):\n",
    "    continue\n",
    "  print(i)\n",
    "\n",
    "  #ground truth pre-pre-processing\n",
    "  ground_truth=whisper2_metric_df.loc[i, 'annotated_groundTruth']\n",
    "  ground_truth=ground_truth.replace('<', '')\n",
    "  ground_truth=ground_truth.replace('>', '')\n",
    "  ground_truth=ground_truth.lower()\n",
    "\n",
    "  #whisper pre-processing\n",
    "  whisper_guess=whisper2_metric_df.loc[i, 'bigWhisperAnnotation']\n",
    "  if(type(whisper_guess)==float):\n",
    "    whisper_guess=\" \"\n",
    "  whisper_guess=remove_punctuation(whisper_guess)\n",
    "  whisper_guess=whisper_guess.lower()\n",
    "\n",
    "  #LITERAL STUFF\n",
    "  literal_string=remove_punctuation(clean_ground_truth(ground_truth,method=\"literal\"))\n",
    "  \n",
    "  \n",
    "  whisper2_metric_df.loc[i, 'literal_2gram_bleu']= sentence_bleu([literal_string.split()], whisper_guess.split(), weights=(1, 1, 0, 0))\n",
    "   \n",
    "\n",
    "  #SEMANTIC STUFF\n",
    "  semantic_string=remove_punctuation(clean_ground_truth(ground_truth,method=\"semantic\"))\n",
    "\n",
    "  whisper2_metric_df.loc[i, 'semantic_2gram_bleu']= sentence_bleu([semantic_string.split()], whisper_guess.split(), weights=(1, 1, 0, 0))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "whisper2_metric_df[\"literal_BERT_precision\"]=None\n",
    "whisper2_metric_df[\"literal_BERT_recall\"]=None\n",
    "whisper2_metric_df[\"literal_BERT_F1\"]=None\n",
    "whisper2_metric_df[\"semantic_BERT_precision\"]=None\n",
    "whisper2_metric_df[\"semantic_BERT_recall\"]=None\n",
    "whisper2_metric_df[\"semantic_BERT_F1\"]=None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "scorer = BERTScorer(model_type='bert-base-uncased')\n",
    "for i in range(whisper2_metric_df.shape[0]):\n",
    "  if(type(whisper2_metric_df.loc[i, 'annotated_groundTruth'])!=str):\n",
    "    continue\n",
    "  print(i)\n",
    "\n",
    "  #ground truth pre-pre-processing\n",
    "  ground_truth=whisper2_metric_df.loc[i, 'annotated_groundTruth']\n",
    "  ground_truth=ground_truth.replace('<', '')\n",
    "  ground_truth=ground_truth.replace('>', '')\n",
    "  ground_truth=ground_truth.lower()\n",
    "\n",
    "  #whisper pre-processing\n",
    "  whisper_guess=whisper2_metric_df.loc[i, 'bigWhisperAnnotation']\n",
    "  if(type(whisper_guess)==float):\n",
    "    whisper_guess=\" \"\n",
    "  whisper_guess=remove_punctuation(whisper_guess)\n",
    "  whisper_guess=whisper_guess.lower()\n",
    "\n",
    "  #LITERAL STUFF\n",
    "  literal_string=remove_punctuation(clean_ground_truth(ground_truth,method=\"literal\"))\n",
    "\n",
    "  literalP, literalR, literalF1= scorer.score([whisper_guess], [literal_string])\n",
    "  \n",
    "  whisper2_metric_df.loc[i,\"literal_BERT_precision\"]=literalP.mean().item()\n",
    "  whisper2_metric_df.loc[i,\"literal_BERT_recall\"]=literalR.mean().item()\n",
    "  whisper2_metric_df.loc[i,\"literal_BERT_F1\"] = literalF1.mean().item()\n",
    "  \n",
    "   \n",
    "\n",
    "  #SEMANTIC STUFF\n",
    "  semantic_string=remove_punctuation(clean_ground_truth(ground_truth,method=\"semantic\"))\n",
    "  \n",
    "  semanticP, semanticR, semanticF1=scorer.score([whisper_guess], [semantic_string])\n",
    "\n",
    "  whisper2_metric_df.loc[i,\"semantic_BERT_precision\"]=semanticP.mean().item()\n",
    "  whisper2_metric_df.loc[i,\"semantic_BERT_recall\"]=semanticR.mean().item()\n",
    "  whisper2_metric_df.loc[i,\"semantic_BERT_F1\"] = semanticF1.mean().item()\n",
    "  \n",
    "  whisper2_metric_df.to_csv(\"<path to separate whisper2 metric sv>\",index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Whisper 3 Metric Calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "whisper3_metric_df=pd.read_csv(\"<path to label_df csv file>\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for i in range(whisper3_metric_df.shape[0]):\n",
    "  if(type(whisper3_metric_df.loc[i, 'annotated_groundTruth'])!=str):\n",
    "    continue\n",
    "  print(i)\n",
    "\n",
    "  #ground truth pre-pre-processing\n",
    "  ground_truth=whisper3_metric_df.loc[i, 'annotated_groundTruth']\n",
    "  ground_truth=ground_truth.replace('<', '')\n",
    "  ground_truth=ground_truth.replace('>', '')\n",
    "  ground_truth=ground_truth.lower()\n",
    "\n",
    "  #whisper pre-processing\n",
    "  whisper_guess=whisper3_metric_df.loc[i, 'bigWhisperAnnotation']\n",
    "  if(type(whisper_guess)==float):\n",
    "    whisper_guess=\" \"\n",
    "  whisper_guess=remove_punctuation(whisper_guess)\n",
    "  whisper_guess=whisper_guess.lower()\n",
    "\n",
    "  #LITERAL STUFF\n",
    "  literal_string=remove_punctuation(clean_ground_truth(ground_truth,method=\"literal\"))\n",
    "  \n",
    "  if(literal_string.strip()==\"\" and whisper_guess.strip()==\"\"):\n",
    "    whisper3_metric_df.loc[i, 'literal_WER']=0.0\n",
    "    whisper3_metric_df.loc[i, 'literal_ref']=0\n",
    "    whisper3_metric_df.loc[i, 'literal_sub']=0.0\n",
    "    whisper3_metric_df.loc[i, 'literal_ins']=0.0\n",
    "    whisper3_metric_df.loc[i, 'literal_del']=0.0\n",
    "    \n",
    "  elif(literal_string.strip()==\"\"):\n",
    "    whisper3_metric_df.loc[i, 'literal_WER']=1.0\n",
    "    whisper3_metric_df.loc[i, 'literal_ref']=len(whisper_guess.split())\n",
    "    whisper3_metric_df.loc[i, 'literal_sub']=1.0\n",
    "    whisper3_metric_df.loc[i, 'literal_ins']=1.0\n",
    "    whisper3_metric_df.loc[i, 'literal_del']=1.0\n",
    "  else:\n",
    "    literal_results=jiwer.process_words(literal_string,whisper_guess)\n",
    "    whisper3_metric_df.loc[i, 'literal_WER']=min(1,literal_results.wer)\n",
    "    whisper3_metric_df.loc[i, 'literal_ref']=len(literal_string.split())\n",
    "    whisper3_metric_df.loc[i, 'literal_sub']=literal_results.substitutions\n",
    "    whisper3_metric_df.loc[i, 'literal_ins']=literal_results.insertions\n",
    "    whisper3_metric_df.loc[i, 'literal_del']=literal_results.deletions\n",
    "\n",
    "\n",
    "  #SEMANTIC STUFF\n",
    "  semantic_string=remove_punctuation(clean_ground_truth(ground_truth,method=\"semantic\"))\n",
    "\n",
    "  if(semantic_string.strip()==\"\" and whisper_guess.strip()==\"\"):\n",
    "    whisper3_metric_df.loc[i, 'semantic_WER']=0.0\n",
    "    whisper3_metric_df.loc[i, 'semantic_ref']=0.0\n",
    "    whisper3_metric_df.loc[i, 'semantic_sub']=0.0\n",
    "    whisper3_metric_df.loc[i, 'semantic_ins']=0.0\n",
    "    whisper3_metric_df.loc[i, 'semantic_del']=0.0\n",
    "    \n",
    "  elif(semantic_string.strip()==\"\"):\n",
    "    whisper3_metric_df.loc[i, 'semantic_WER']=1.0\n",
    "    whisper3_metric_df.loc[i, 'semantic_ref']=len(whisper_guess.split())\n",
    "    whisper3_metric_df.loc[i, 'semantic_sub']=1.0\n",
    "    whisper3_metric_df.loc[i, 'semantic_ins']=1.0\n",
    "    whisper3_metric_df.loc[i, 'semantic_del']=1.0\n",
    "  else:\n",
    "    semantic_results=jiwer.process_words(semantic_string,whisper_guess)\n",
    "    whisper3_metric_df.loc[i, 'semantic_WER']=min(1,semantic_results.wer)\n",
    "    whisper3_metric_df.loc[i, 'semantic_ref']=len(semantic_string.split())\n",
    "    whisper3_metric_df.loc[i, 'semantic_sub']=semantic_results.substitutions\n",
    "    whisper3_metric_df.loc[i, 'semantic_ins']=semantic_results.insertions\n",
    "    whisper3_metric_df.loc[i, 'semantic_del']=semantic_results.deletions\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for i in range(whisper3_metric_df.shape[0]):\n",
    "  if(type(whisper3_metric_df.loc[i, 'annotated_groundTruth'])!=str):\n",
    "    continue\n",
    "  print(i)\n",
    "\n",
    "  #ground truth pre-pre-processing\n",
    "  ground_truth=whisper3_metric_df.loc[i, 'annotated_groundTruth']\n",
    "  ground_truth=ground_truth.replace('<', '')\n",
    "  ground_truth=ground_truth.replace('>', '')\n",
    "  ground_truth=ground_truth.lower()\n",
    "\n",
    "  #whisper pre-processing\n",
    "  whisper_guess=whisper3_metric_df.loc[i, 'bigWhisperAnnotation']\n",
    "  if(type(whisper_guess)==float):\n",
    "    whisper_guess=\" \"\n",
    "  whisper_guess=remove_punctuation(whisper_guess)\n",
    "  whisper_guess=whisper_guess.lower()\n",
    "\n",
    "  #LITERAL STUFF\n",
    "  literal_string=remove_punctuation(clean_ground_truth(ground_truth,method=\"literal\"))\n",
    "  \n",
    "  \n",
    "  whisper3_metric_df.loc[i, 'literal_2gram_bleu']= sentence_bleu([literal_string.split()], whisper_guess.split(), weights=(1, 1, 0, 0))\n",
    "   \n",
    "\n",
    "  #SEMANTIC STUFF\n",
    "  semantic_string=remove_punctuation(clean_ground_truth(ground_truth,method=\"semantic\"))\n",
    "\n",
    "  whisper3_metric_df.loc[i, 'semantic_2gram_bleu']= sentence_bleu([semantic_string.split()], whisper_guess.split(), weights=(1, 1, 0, 0))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "whisper3_metric_df[\"literal_BERT_precision\"]=None\n",
    "whisper3_metric_df[\"literal_BERT_recall\"]=None\n",
    "whisper3_metric_df[\"literal_BERT_F1\"]=None\n",
    "whisper3_metric_df[\"semantic_BERT_precision\"]=None\n",
    "whisper3_metric_df[\"semantic_BERT_recall\"]=None\n",
    "whisper3_metric_df[\"semantic_BERT_F1\"]=None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "scorer = BERTScorer(model_type='bert-base-uncased')\n",
    "for i in range(whisper3_metric_df.shape[0]):\n",
    "  if(type(whisper3_metric_df.loc[i, 'annotated_groundTruth'])!=str):\n",
    "    continue\n",
    "  print(i)\n",
    "\n",
    "  #ground truth pre-pre-processing\n",
    "  ground_truth=whisper3_metric_df.loc[i, 'annotated_groundTruth']\n",
    "  ground_truth=ground_truth.replace('<', '')\n",
    "  ground_truth=ground_truth.replace('>', '')\n",
    "  ground_truth=ground_truth.lower()\n",
    "\n",
    "  #whisper pre-processing\n",
    "  whisper_guess=whisper3_metric_df.loc[i, 'bigWhisperAnnotation']\n",
    "  if(type(whisper_guess)==float):\n",
    "    whisper_guess=\" \"\n",
    "  whisper_guess=remove_punctuation(whisper_guess)\n",
    "  whisper_guess=whisper_guess.lower()\n",
    "\n",
    "  #LITERAL STUFF\n",
    "  literal_string=remove_punctuation(clean_ground_truth(ground_truth,method=\"literal\"))\n",
    "\n",
    "  literalP, literalR, literalF1= scorer.score([whisper_guess], [literal_string])\n",
    "  \n",
    "  whisper3_metric_df.loc[i,\"literal_BERT_precision\"]=literalP.mean().item()\n",
    "  whisper3_metric_df.loc[i,\"literal_BERT_recall\"]=literalR.mean().item()\n",
    "  whisper3_metric_df.loc[i,\"literal_BERT_F1\"] = literalF1.mean().item()\n",
    "  \n",
    "   \n",
    "\n",
    "  #SEMANTIC STUFF\n",
    "  semantic_string=remove_punctuation(clean_ground_truth(ground_truth,method=\"semantic\"))\n",
    "  \n",
    "  semanticP, semanticR, semanticF1=scorer.score([whisper_guess], [semantic_string])\n",
    "\n",
    "  whisper3_metric_df.loc[i,\"semantic_BERT_precision\"]=semanticP.mean().item()\n",
    "  whisper3_metric_df.loc[i,\"semantic_BERT_recall\"]=semanticR.mean().item()\n",
    "  whisper3_metric_df.loc[i,\"semantic_BERT_F1\"] = semanticF1.mean().item()\n",
    "  \n",
    "  whisper3_metric_df.to_csv(\"<path to separate whisper2 metric sv>\",index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Whisper 2 Results Graphing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Literal Overview Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "literal_WER=[]\n",
    "literal_2gram_bleu=[]\n",
    "literal_BERT_F1=[]\n",
    "\n",
    "for i in [\"YesStutteredWords\",\"manual_fluent\",\"manual_prolongation\",\"manual_block\",\"manual_soundRep\",\"manual_wordRep\",\"manual_interject\"]:\n",
    "    if(i==\"YesStutteredWords\"):\n",
    "        group_df=whisper2_metric_df.groupby([\"manual_fluent\"]).mean(numeric_only=True)\n",
    "        metrics=group_df.iloc[0,:][[\"literal_WER\",\"literal_2gram_bleu\",\"literal_BERT_F1\"]].values\n",
    "        \n",
    "        literal_WER+=[metrics[0]]\n",
    "        literal_2gram_bleu+=[metrics[1]]\n",
    "        literal_BERT_F1+=[metrics[2]]\n",
    "    else:\n",
    "        group_df=whisper2_metric_df.groupby([i]).mean(numeric_only=True)\n",
    "        metrics=group_df.iloc[1,:][[\"literal_WER\",\"literal_2gram_bleu\",\"literal_BERT_F1\"]].values\n",
    "        \n",
    "        literal_WER+=[metrics[0]]\n",
    "        literal_2gram_bleu+=[metrics[1]]\n",
    "        literal_BERT_F1+=[metrics[2]]\n",
    "\n",
    "literal_WER=[round(j, 3) for j in literal_WER ]\n",
    "literal_2gram_bleu=[round(j, 3) for j in literal_2gram_bleu ]\n",
    "literal_BERT_F1=[round(j, 3) for j in literal_BERT_F1 ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "speech_categories = (\"Stutter\",\"Fluent\",\"Prolongation\",\"Block\",\"Sound\\nRepetition\",\"Word\\nRepetition\",\"Interjection\")\n",
    "speech_metrics = {\n",
    "    'Literal WER': tuple(literal_WER),\n",
    "    #'Literal_BLEU-2': tuple(literal_2gram_bleu),\n",
    "    #'Literal_BERT_F1': tuple(literal_BERT_F1),\n",
    "}\n",
    "\n",
    "x = np.arange(len(speech_categories))  # the label locations\n",
    "width = 0.5 # the width of the bars\n",
    "multiplier = 0\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "for attribute, measurement in speech_metrics.items():\n",
    "    offset = width * multiplier\n",
    "    rects = ax.bar(x + offset, measurement, width, label=attribute,color=\"darkorange\")\n",
    "    ax.bar_label(rects, padding=3,fontsize='large')\n",
    "    multiplier += 1\n",
    "\n",
    "# Add some text for labels, title and custom x-axis tick labels, etc.\n",
    "ax.set_ylabel('Metric Score',fontsize='large')\n",
    "ax.set_title('Speech to Text WER for Literal\\n Transcriptions of each Stuttering Type',fontsize='x-large')\n",
    "ax.set_xticks(x, speech_categories,fontsize='large',rotation=90)\n",
    "ax.legend(loc='upper left', ncols=1,fontsize='large')\n",
    "ax.set_ylim(0, 0.6)\n",
    "\n",
    "fig.set_figheight(5)\n",
    "fig.set_figwidth(7)\n",
    "\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "speech_categories = (\"Stutter\",\"Fluent\",\"Prolongation\",\"Block\",\"Sound\\nRepetition\",\"Word\\nRepetition\",\"Interjection\")\n",
    "speech_metrics = {\n",
    "    #'Literal WER': tuple(literal_WER),\n",
    "    'Literal BLEU-2': tuple(literal_2gram_bleu),\n",
    "    #'Literal_BERT_F1': tuple(literal_BERT_F1),\n",
    "}\n",
    "\n",
    "x = np.arange(len(speech_categories))  # the label locations\n",
    "width = 0.5  # the width of the bars\n",
    "multiplier = 0\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "for attribute, measurement in speech_metrics.items():\n",
    "    offset = width * multiplier\n",
    "    rects = ax.bar(x + offset, measurement, width, label=attribute,color=\"green\")\n",
    "    ax.bar_label(rects, padding=3,fontsize='large')\n",
    "    multiplier += 1\n",
    "\n",
    "# Add some text for labels, title and custom x-axis tick labels, etc.\n",
    "ax.set_ylabel('Metric Score',fontsize='large')\n",
    "ax.set_title('Speech to Text BLEU-2 for Literal\\n Transcriptions of each Stuttering Type',fontsize='x-large')\n",
    "ax.set_xticks(x, speech_categories,fontsize='large',rotation=90)\n",
    "ax.legend(loc='upper left', ncols=1,fontsize='large')\n",
    "ax.set_ylim(0, 0.8)\n",
    "\n",
    "fig.set_figheight(5)\n",
    "fig.set_figwidth(7)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "speech_categories = (\"Stutter\",\"Fluent\",\"Prolongation\",\"Block\",\"Sound\\nRepetition\",\"Word\\nRepetition\",\"Interjection\")\n",
    "speech_metrics = {\n",
    "    #'Literal WER': tuple(literal_WER),\n",
    "    #'Literal_2gram_BLEU': tuple(literal_2gram_bleu),\n",
    "    'Literal BERTScore F1': tuple(literal_BERT_F1),\n",
    "}\n",
    "\n",
    "x = np.arange(len(speech_categories))  # the label locations\n",
    "width = 0.5  # the width of the bars\n",
    "multiplier = 0\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "for attribute, measurement in speech_metrics.items():\n",
    "    offset = width * multiplier\n",
    "    rects = ax.bar(x + offset, measurement, width, label=attribute,color=\"C0\")\n",
    "    ax.bar_label(rects, padding=3,fontsize='large')\n",
    "    multiplier += 1\n",
    "\n",
    "# Add some text for labels, title and custom x-axis tick labels, etc.\n",
    "ax.set_ylabel('Metric Score',fontsize='large')\n",
    "ax.set_title('Speech to Text BERTScore F1 for Literal\\n Transcriptions of each Stuttering Type',fontsize='x-large')\n",
    "ax.set_xticks(x, speech_categories,fontsize='large',rotation=90)\n",
    "ax.legend(loc='upper left', ncols=1,fontsize='large')\n",
    "ax.set_ylim(0, 1.05)\n",
    "\n",
    "fig.set_figheight(5)\n",
    "fig.set_figwidth(7)\n",
    "\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Semantic Overview Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "semantic_WER=[]\n",
    "semantic_2gram_bleu=[]\n",
    "semantic_BERT_F1=[]\n",
    "\n",
    "for i in [\"YesStutteredWords\",\"manual_fluent\",\"manual_prolongation\",\"manual_block\",\"manual_soundRep\",\"manual_wordRep\",\"manual_interject\"]:\n",
    "    if(i==\"YesStutteredWords\"):\n",
    "        group_df=whisper2_metric_df.groupby([\"manual_fluent\"]).mean(numeric_only=True)\n",
    "        metrics=group_df.iloc[0,:][[\"semantic_WER\",\"semantic_2gram_bleu\",\"semantic_BERT_F1\"]].values\n",
    "        \n",
    "        semantic_WER+=[metrics[0]]\n",
    "        semantic_2gram_bleu+=[metrics[1]]\n",
    "        semantic_BERT_F1+=[metrics[2]]\n",
    "    else:\n",
    "        group_df=whisper2_metric_df.groupby([i]).mean(numeric_only=True)\n",
    "        metrics=group_df.iloc[1,:][[\"semantic_WER\",\"semantic_2gram_bleu\",\"semantic_BERT_F1\"]].values\n",
    "        \n",
    "        semantic_WER+=[metrics[0]]\n",
    "        semantic_2gram_bleu+=[metrics[1]]\n",
    "        semantic_BERT_F1+=[metrics[2]]\n",
    "\n",
    "semantic_WER=[round(j, 3) for j in semantic_WER ]\n",
    "semantic_2gram_bleu=[round(j, 3) for j in semantic_2gram_bleu ]\n",
    "semantic_BERT_F1=[round(j, 3) for j in semantic_BERT_F1 ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "speech_categories = (\"Stutter\",\"Fluent\",\"Prolongation\",\"Block\",\"Sound\\nRepetition\",\"Word\\nRepetition\",\"Interjection\")\n",
    "speech_metrics = {\n",
    "    'Semantic WER': tuple(semantic_WER),\n",
    "    #'Semantic_2gram_BLEU': tuple(semantic_2gram_bleu),\n",
    "    #'Semantic_BERT_F1': tuple(semantic_BERT_F1),\n",
    "}\n",
    "\n",
    "x = np.arange(len(speech_categories))  # the label locations\n",
    "width = 0.5 # the width of the bars\n",
    "multiplier = 0\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "for attribute, measurement in speech_metrics.items():\n",
    "    offset = width * multiplier\n",
    "    rects = ax.bar(x + offset, measurement, width, label=attribute,color=\"darkorange\")\n",
    "    ax.bar_label(rects, padding=3,fontsize='large')\n",
    "    multiplier += 1\n",
    "\n",
    "# Add some text for labels, title and custom x-axis tick labels, etc.\n",
    "ax.set_ylabel('Metric Score',fontsize='large')\n",
    "ax.set_title('Speech to Text WER for Semantic\\n Transcriptions of each Stuttering Type',fontsize='x-large')\n",
    "ax.set_xticks(x, speech_categories,fontsize='large',rotation=90)\n",
    "ax.legend(loc='upper left', ncols=1,fontsize='large')\n",
    "ax.set_ylim(0, 0.6)\n",
    "\n",
    "fig.set_figheight(5)\n",
    "fig.set_figwidth(7)\n",
    "\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "speech_categories = (\"Stutter\",\"Fluent\",\"Prolongation\",\"Block\",\"Sound\\nRepetition\",\"Word\\nRepetition\",\"Interjection\")\n",
    "speech_metrics = {\n",
    "    #'Semantic WER': tuple(semantic_WER),\n",
    "    'Semantic BLEU-2': tuple(semantic_2gram_bleu),\n",
    "    #'Semantic_BERT_F1': tuple(semantic_BERT_F1),\n",
    "}\n",
    "\n",
    "x = np.arange(len(speech_categories))  # the label locations\n",
    "width = 0.5  # the width of the bars\n",
    "multiplier = 0\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "for attribute, measurement in speech_metrics.items():\n",
    "    offset = width * multiplier\n",
    "    rects = ax.bar(x + offset, measurement, width, label=attribute,color=\"green\")\n",
    "    ax.bar_label(rects, padding=3,fontsize='large')\n",
    "    multiplier += 1\n",
    "\n",
    "# Add some text for labels, title and custom x-axis tick labels, etc.\n",
    "ax.set_ylabel('Metric Score',fontsize='large')\n",
    "ax.set_title('Speech to Text BLEU-2 for Semantic\\n Transcriptions of each Stuttering Type',fontsize='x-large')\n",
    "ax.set_xticks(x, speech_categories,fontsize='large',rotation=90)\n",
    "ax.legend(loc='upper left', ncols=1,fontsize='large')\n",
    "ax.set_ylim(0, 0.8)\n",
    "\n",
    "fig.set_figheight(5)\n",
    "fig.set_figwidth(7)\n",
    "\n",
    "\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "speech_categories = (\"Stutter\",\"Fluent\",\"Prolongation\",\"Block\",\"Sound\\nRepetition\",\"Word\\nRepetition\",\"Interjection\")\n",
    "speech_metrics = {\n",
    "    #'Semantic WER': tuple(semantic_WER),\n",
    "    #'Semantic_2gram_BLEU': tuple(semantic_2gram_bleu),\n",
    "    'Semantic BERT F1': tuple(semantic_BERT_F1),\n",
    "}\n",
    "\n",
    "x = np.arange(len(speech_categories))  # the label locations\n",
    "width = 0.5  # the width of the bars\n",
    "multiplier = 0\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "for attribute, measurement in speech_metrics.items():\n",
    "    offset = width * multiplier\n",
    "    rects = ax.bar(x + offset, measurement, width, label=attribute,color=\"C0\")\n",
    "    ax.bar_label(rects, padding=3,fontsize='large')\n",
    "    multiplier += 1\n",
    "\n",
    "# Add some text for labels, title and custom x-axis tick labels, etc.\n",
    "ax.set_ylabel('Metric Score',fontsize='large')\n",
    "ax.set_title('Speech to Text BERTScore F1 for Semantic\\n Transcriptions of each Stuttering Type',fontsize='x-large')\n",
    "ax.set_xticks(x, speech_categories,fontsize='large',rotation=90)\n",
    "ax.legend(loc='upper left', ncols=1,fontsize='large')\n",
    "ax.set_ylim(0, 1.05)\n",
    "\n",
    "fig.set_figheight(5)\n",
    "fig.set_figwidth(7)\n",
    "\n",
    "\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Literal Insertions, deletions, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_avg_literal_wer_comp(name: str, metric: str):\n",
    "    if(name==\"YesStutteredWords\"):\n",
    "        filtered_df=whisper2_metric_df[whisper2_metric_df[\"manual_fluent\"]==0]\n",
    "    else:\n",
    "        filtered_df=whisper2_metric_df[whisper2_metric_df[name]==1]\n",
    "\n",
    "    filtered_df=filtered_df.dropna(subset=[metric])\n",
    "    relative_metric_arr=filtered_df[metric]/filtered_df[\"literal_ref\"]\n",
    "    return relative_metric_arr.mean()\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "literal_sub=[]\n",
    "literal_ins=[]\n",
    "literal_del=[]\n",
    "\n",
    "\n",
    "for i in [\"YesStutteredWords\",\"manual_fluent\",\"manual_prolongation\",\"manual_block\",\"manual_soundRep\",\"manual_wordRep\",\"manual_interject\"]:\n",
    "    if(i==\"YesStutteredWords\"):\n",
    "        literal_sub+=[get_avg_literal_wer_comp(\"YesStutteredWords\",\"literal_sub\")]\n",
    "        literal_ins+=[get_avg_literal_wer_comp(\"YesStutteredWords\",\"literal_ins\")]\n",
    "        literal_del+=[get_avg_literal_wer_comp(\"YesStutteredWords\",\"literal_del\")]\n",
    "    else:\n",
    "        literal_sub+=[get_avg_literal_wer_comp(i,\"literal_sub\")]\n",
    "        literal_ins+=[get_avg_literal_wer_comp(i,\"literal_ins\")]\n",
    "        literal_del+=[get_avg_literal_wer_comp(i,\"literal_del\")]\n",
    "\n",
    "literal_sub=[round(j, 3) for j in literal_sub ]\n",
    "literal_ins=[round(j, 3) for j in literal_ins ]\n",
    "literal_del=[round(j, 3) for j in literal_del ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "literal_sub=[]\n",
    "literal_ins=[]\n",
    "literal_del=[]\n",
    "\n",
    "\n",
    "for i in [\"YesStutteredWords\",\"manual_fluent\",\"manual_prolongation\",\"manual_block\",\"manual_soundRep\",\"manual_wordRep\",\"manual_interject\"]:\n",
    "    literal_sub+=[get_avg_literal_wer_comp(i,\"literal_sub\")]\n",
    "    literal_ins+=[get_avg_literal_wer_comp(i,\"literal_ins\")]\n",
    "    literal_del+=[get_avg_literal_wer_comp(i,\"literal_del\")]\n",
    "\n",
    "literal_sub=[round(j, 3) for j in literal_sub ]\n",
    "literal_ins=[round(j, 3) for j in literal_ins ]\n",
    "literal_del=[round(j, 3) for j in literal_del ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "speech_categories = (\"Stutter\",\"Fluent\",\"Prolongation\",\"Block\",\"Sound\\nRepetition\",\"Word\\nRepetition\",\"Interjection\")\n",
    "speech_metrics = {\n",
    "    'Literal Substitutions': tuple(literal_sub),\n",
    "    'Literal Insertions': tuple(literal_ins),\n",
    "    'Literal Deletions': tuple(literal_del),\n",
    "}\n",
    "\n",
    "x = np.arange(len(speech_categories))  # the label locations\n",
    "width = 0.3  # the width of the bars\n",
    "multiplier = 0\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "for attribute, measurement in speech_metrics.items():\n",
    "    offset = width * multiplier\n",
    "    rects = ax.bar(x + offset, measurement, width, label=attribute)\n",
    "    ax.bar_label(rects, padding=3)\n",
    "    multiplier += 1\n",
    "\n",
    "# Add some text for labels, title and custom x-axis tick labels, etc.\n",
    "ax.set_ylabel('Metric Score')\n",
    "ax.set_title('Speech to Text Relative Substitutions, Insertions, and Deletions for Literal Transcriptions of each Stuttering Type',fontsize='x-large')\n",
    "ax.set_xticks(x+width, speech_categories,fontsize='large')\n",
    "ax.legend(loc='upper left', ncols=1,fontsize='x-large')\n",
    "ax.set_ylim(0, 0.55)\n",
    "\n",
    "fig.set_figheight(5)\n",
    "fig.set_figwidth(16)\n",
    "\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Semantic Subs, Insertions, deletions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_avg_semantic_wer_comp(name: str, metric: str):\n",
    "    if(name==\"YesStutteredWords\"):\n",
    "        filtered_df=whisper2_metric_df[whisper2_metric_df[\"manual_fluent\"]==0]\n",
    "    else:\n",
    "        filtered_df=whisper2_metric_df[whisper2_metric_df[name]==1]\n",
    "\n",
    "    filtered_df=filtered_df.dropna(subset=[metric])\n",
    "    relative_metric_arr=filtered_df[metric]/filtered_df[\"semantic_ref\"]\n",
    "    return relative_metric_arr.mean()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "semantic_sub=[]\n",
    "semantic_ins=[]\n",
    "semantic_del=[]\n",
    "\n",
    "\n",
    "for i in [\"YesStutteredWords\",\"manual_fluent\",\"manual_prolongation\",\"manual_block\",\"manual_soundRep\",\"manual_wordRep\",\"manual_interject\"]:\n",
    "    if(i==\"YesStutteredWords\"):\n",
    "        semantic_sub+=[get_avg_semantic_wer_comp(\"YesStutteredWords\",\"semantic_sub\")]\n",
    "        semantic_ins+=[get_avg_semantic_wer_comp(\"YesStutteredWords\",\"semantic_ins\")]\n",
    "        semantic_del+=[get_avg_semantic_wer_comp(\"YesStutteredWords\",\"semantic_del\")]\n",
    "    else:\n",
    "        semantic_sub+=[get_avg_semantic_wer_comp(i,\"semantic_sub\")]\n",
    "        semantic_ins+=[get_avg_semantic_wer_comp(i,\"semantic_ins\")]\n",
    "        semantic_del+=[get_avg_semantic_wer_comp(i,\"semantic_del\")]\n",
    "\n",
    "semantic_sub=[round(j, 3) for j in semantic_sub ]\n",
    "semantic_ins=[round(j, 3) for j in semantic_ins ]\n",
    "semantic_del=[round(j, 3) for j in semantic_del ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "speech_categories = (\"Stutter\",\"Fluent\",\"Prolongation\",\"Block\",\"Sound\\nRepetition\",\"Word\\nRepetition\",\"Interjection\")\n",
    "speech_metrics = {\n",
    "    'Semantic Substitutions': tuple(semantic_sub),\n",
    "    'Semantic Insertions': tuple(semantic_ins),\n",
    "    'Semantic Deletions': tuple(semantic_del),\n",
    "}\n",
    "\n",
    "x = np.arange(len(speech_categories))  # the label locations\n",
    "width = 0.3  # the width of the bars\n",
    "multiplier = 0\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "for attribute, measurement in speech_metrics.items():\n",
    "    offset = width * multiplier\n",
    "    rects = ax.bar(x + offset, measurement, width, label=attribute)\n",
    "    ax.bar_label(rects, padding=3)\n",
    "    multiplier += 1\n",
    "\n",
    "# Add some text for labels, title and custom x-axis tick labels, etc.\n",
    "ax.set_ylabel('Metric Score')\n",
    "ax.set_title('Speech to Text Relative Substitutions, Insertions, and Deletions for Semantic Transcriptions of each Stuttering Type',fontsize='x-large')\n",
    "ax.set_xticks(x+width, speech_categories,fontsize='large')\n",
    "ax.legend(loc='upper left', ncols=1,fontsize='x-large')\n",
    "ax.set_ylim(0, 0.65)\n",
    "\n",
    "fig.set_figheight(5)\n",
    "fig.set_figwidth(16)\n",
    "\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparing Sematnic vs Literal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_nd_array(columnval):\n",
    "    newdf=whisper2_metric_df[whisper2_metric_df[\"manual_fluent\"]!=1]\n",
    "    nparr=newdf[columnval].to_numpy()\n",
    "    nparr=nparr[~np.isnan(nparr)]\n",
    "    return nparr\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "speech_categories = (\"Literal\",\"Semantic\")\n",
    "speech_metrics = {\n",
    "    'Average WER': (np.mean(get_nd_array(\"literal_WER\")),np.mean(get_nd_array(\"semantic_WER\"))),\n",
    "    #'Average 2 Gram BLEU': (np.mean(get_nd_array(\"literal_2gram_bleu\")),np.mean(get_nd_array(\"semantic_2gram_bleu\"))),\n",
    "    #'Average BERT F1': (np.mean(get_nd_array(\"literal_BERT_F1\")),np.mean(get_nd_array(\"semantic_BERT_F1\"))),\n",
    "}\n",
    "\n",
    "x = np.arange(len(speech_categories))  # the label locations\n",
    "width = 0.5  # the width of the bars\n",
    "multiplier = 0\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "for attribute, measurement in speech_metrics.items():\n",
    "    offset = width * multiplier\n",
    "    rects = ax.bar(x, measurement, width, label=attribute,color=\"darkorange\")\n",
    "    ax.bar_label(rects, padding=1,fontsize='x-large',fmt='%.2f')\n",
    "    multiplier += 1\n",
    "\n",
    "# Add some text for labels, title and custom x-axis tick labels, etc.\n",
    "ax.set_ylabel('Metric Score',fontsize=\"x-large\")\n",
    "ax.set_title(\"WER of Semantic vs Literal\\n Transcription Speech to Text\\n for Stuttered Speech\",fontsize='x-large')\n",
    "ax.set_xticks(x, speech_categories,fontsize='x-large')\n",
    "ax.legend(loc='upper left', ncols=3,fontsize='x-large')\n",
    "ax.set_ylim(0, 0.6)\n",
    "\n",
    "fig.set_figheight(6)\n",
    "fig.set_figwidth(4)\n",
    "\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "speech_categories = (\"Literal\",\"Semantic\")\n",
    "speech_metrics = {\n",
    "    #'Average WER': (np.mean(get_nd_array(\"literal_WER\")),np.mean(get_nd_array(\"semantic_WER\"))),\n",
    "    'Average 2 Gram BLEU': (np.mean(get_nd_array(\"literal_2gram_bleu\")),np.mean(get_nd_array(\"semantic_2gram_bleu\"))),\n",
    "    #'Average BERT F1': (np.mean(get_nd_array(\"literal_BERT_F1\")),np.mean(get_nd_array(\"semantic_BERT_F1\"))),\n",
    "}\n",
    "\n",
    "x = np.arange(len(speech_categories))  # the label locations\n",
    "width = 0.6  # the width of the bars\n",
    "multiplier = 0\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "for attribute, measurement in speech_metrics.items():\n",
    "    offset = width * multiplier\n",
    "    rects = ax.bar(x, measurement, width, label=attribute,color=\"green\")\n",
    "    ax.bar_label(rects, padding=1,fontsize='x-large',fmt='%.2f')\n",
    "    multiplier += 1\n",
    "\n",
    "# Add some text for labels, title and custom x-axis tick labels, etc.\n",
    "ax.set_ylabel('Metric Score',fontsize=\"x-large\")\n",
    "ax.set_title(\"BLEU-2 of Semantic vs Literal\\n Transcription Speech to Text\\n for Stuttered Speech\",fontsize='x-large')\n",
    "ax.set_xticks(x, speech_categories,fontsize='x-large')\n",
    "ax.legend(loc='upper left', ncols=3,fontsize='x-large')\n",
    "ax.set_ylim(0, 0.55)\n",
    "\n",
    "fig.set_figheight(6)\n",
    "fig.set_figwidth(4)\n",
    "\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "speech_categories = (\"Literal\",\"Semantic\")\n",
    "speech_metrics = {\n",
    "    #'Average WER': (np.mean(get_nd_array(\"literal_WER\")),np.mean(get_nd_array(\"semantic_WER\"))),\n",
    "    #'Average 2 Gram BLEU': (np.mean(get_nd_array(\"literal_2gram_bleu\")),np.mean(get_nd_array(\"semantic_2gram_bleu\"))),\n",
    "    'Average BERT F1': (np.mean(get_nd_array(\"literal_BERT_F1\")),np.mean(get_nd_array(\"semantic_BERT_F1\"))),\n",
    "}\n",
    "\n",
    "x = np.arange(len(speech_categories))  # the label locations\n",
    "width = 0.6  # the width of the bars\n",
    "multiplier = 0\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "for attribute, measurement in speech_metrics.items():\n",
    "    offset = width * multiplier\n",
    "    rects = ax.bar(x, measurement, width, label=attribute,color=\"C0\")\n",
    "    ax.bar_label(rects, padding=1,fontsize='x-large',fmt='%.2f')\n",
    "    multiplier += 1\n",
    "\n",
    "# Add some text for labels, title and custom x-axis tick labels, etc.\n",
    "ax.set_ylabel('Metric Score',fontsize=\"x-large\")\n",
    "ax.set_title(\"BERT of Semantic vs Literal\\n Transcription Speech to Text\\n for Stuttered Speech\",fontsize='x-large')\n",
    "ax.set_xticks(x, speech_categories,fontsize='x-large')\n",
    "ax.legend(loc='upper left', ncols=3,fontsize='x-large')\n",
    "ax.set_ylim(0, 0.9)\n",
    "\n",
    "fig.set_figheight(6)\n",
    "fig.set_figwidth(4)\n",
    "\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Whisper 3 Results Graphing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Literal Overview Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "literal_WER=[]\n",
    "literal_2gram_bleu=[]\n",
    "literal_BERT_F1=[]\n",
    "\n",
    "for i in [\"YesStutteredWords\",\"manual_fluent\",\"manual_prolongation\",\"manual_block\",\"manual_soundRep\",\"manual_wordRep\",\"manual_interject\"]:\n",
    "    if(i==\"YesStutteredWords\"):\n",
    "        group_df=whisper3_metric_df.groupby([\"manual_fluent\"]).mean(numeric_only=True)\n",
    "        metrics=group_df.iloc[0,:][[\"literal_WER\",\"literal_2gram_bleu\",\"literal_BERT_F1\"]].values\n",
    "        \n",
    "        literal_WER+=[metrics[0]]\n",
    "        literal_2gram_bleu+=[metrics[1]]\n",
    "        literal_BERT_F1+=[metrics[2]]\n",
    "    else:\n",
    "        group_df=whisper3_metric_df.groupby([i]).mean(numeric_only=True)\n",
    "        metrics=group_df.iloc[1,:][[\"literal_WER\",\"literal_2gram_bleu\",\"literal_BERT_F1\"]].values\n",
    "        \n",
    "        literal_WER+=[metrics[0]]\n",
    "        literal_2gram_bleu+=[metrics[1]]\n",
    "        literal_BERT_F1+=[metrics[2]]\n",
    "\n",
    "literal_WER=[round(j, 3) for j in literal_WER ]\n",
    "literal_2gram_bleu=[round(j, 3) for j in literal_2gram_bleu ]\n",
    "literal_BERT_F1=[round(j, 3) for j in literal_BERT_F1 ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "speech_categories = (\"Stutter\",\"Fluent\",\"Prolongation\",\"Block\",\"Sound\\nRepetition\",\"Word\\nRepetition\",\"Interjection\")\n",
    "speech_metrics = {\n",
    "    'Literal WER': tuple(literal_WER),\n",
    "    #'Literal_BLEU-2': tuple(literal_2gram_bleu),\n",
    "    #'Literal_BERT_F1': tuple(literal_BERT_F1),\n",
    "}\n",
    "\n",
    "x = np.arange(len(speech_categories))  # the label locations\n",
    "width = 0.5 # the width of the bars\n",
    "multiplier = 0\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "for attribute, measurement in speech_metrics.items():\n",
    "    offset = width * multiplier\n",
    "    rects = ax.bar(x + offset, measurement, width, label=attribute,color=\"darkorange\")\n",
    "    ax.bar_label(rects, padding=3,fontsize='large')\n",
    "    multiplier += 1\n",
    "\n",
    "# Add some text for labels, title and custom x-axis tick labels, etc.\n",
    "ax.set_ylabel('Metric Score',fontsize='large')\n",
    "ax.set_title('Speech to Text WER for Literal\\n Transcriptions of each Stuttering Type',fontsize='x-large')\n",
    "ax.set_xticks(x, speech_categories,fontsize='large',rotation=90)\n",
    "ax.legend(loc='upper left', ncols=1,fontsize='large')\n",
    "ax.set_ylim(0, 0.6)\n",
    "\n",
    "fig.set_figheight(5)\n",
    "fig.set_figwidth(7)\n",
    "\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "speech_categories = (\"Stutter\",\"Fluent\",\"Prolongation\",\"Block\",\"Sound\\nRepetition\",\"Word\\nRepetition\",\"Interjection\")\n",
    "speech_metrics = {\n",
    "    #'Literal WER': tuple(literal_WER),\n",
    "    'Literal BLEU-2': tuple(literal_2gram_bleu),\n",
    "    #'Literal_BERT_F1': tuple(literal_BERT_F1),\n",
    "}\n",
    "\n",
    "x = np.arange(len(speech_categories))  # the label locations\n",
    "width = 0.5  # the width of the bars\n",
    "multiplier = 0\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "for attribute, measurement in speech_metrics.items():\n",
    "    offset = width * multiplier\n",
    "    rects = ax.bar(x + offset, measurement, width, label=attribute,color=\"green\")\n",
    "    ax.bar_label(rects, padding=3,fontsize='large')\n",
    "    multiplier += 1\n",
    "\n",
    "# Add some text for labels, title and custom x-axis tick labels, etc.\n",
    "ax.set_ylabel('Metric Score',fontsize='large')\n",
    "ax.set_title('Speech to Text BLEU-2 for Literal\\n Transcriptions of each Stuttering Type',fontsize='x-large')\n",
    "ax.set_xticks(x, speech_categories,fontsize='large',rotation=90)\n",
    "ax.legend(loc='upper left', ncols=1,fontsize='large')\n",
    "ax.set_ylim(0, 0.8)\n",
    "\n",
    "fig.set_figheight(5)\n",
    "fig.set_figwidth(7)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "speech_categories = (\"Stutter\",\"Fluent\",\"Prolongation\",\"Block\",\"Sound\\nRepetition\",\"Word\\nRepetition\",\"Interjection\")\n",
    "speech_metrics = {\n",
    "    #'Literal WER': tuple(literal_WER),\n",
    "    #'Literal_2gram_BLEU': tuple(literal_2gram_bleu),\n",
    "    'Literal BERTScore F1': tuple(literal_BERT_F1),\n",
    "}\n",
    "\n",
    "x = np.arange(len(speech_categories))  # the label locations\n",
    "width = 0.5  # the width of the bars\n",
    "multiplier = 0\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "for attribute, measurement in speech_metrics.items():\n",
    "    offset = width * multiplier\n",
    "    rects = ax.bar(x + offset, measurement, width, label=attribute,color=\"C0\")\n",
    "    ax.bar_label(rects, padding=3,fontsize='large')\n",
    "    multiplier += 1\n",
    "\n",
    "# Add some text for labels, title and custom x-axis tick labels, etc.\n",
    "ax.set_ylabel('Metric Score',fontsize='large')\n",
    "ax.set_title('Speech to Text BERTScore F1 for Literal\\n Transcriptions of each Stuttering Type',fontsize='x-large')\n",
    "ax.set_xticks(x, speech_categories,fontsize='large',rotation=90)\n",
    "ax.legend(loc='upper left', ncols=1,fontsize='large')\n",
    "ax.set_ylim(0, 1.05)\n",
    "\n",
    "fig.set_figheight(5)\n",
    "fig.set_figwidth(7)\n",
    "\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Semantic Overview Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "semantic_WER=[]\n",
    "semantic_2gram_bleu=[]\n",
    "semantic_BERT_F1=[]\n",
    "\n",
    "for i in [\"YesStutteredWords\",\"manual_fluent\",\"manual_prolongation\",\"manual_block\",\"manual_soundRep\",\"manual_wordRep\",\"manual_interject\"]:\n",
    "    if(i==\"YesStutteredWords\"):\n",
    "        group_df=whisper3_metric_df.groupby([\"manual_fluent\"]).mean(numeric_only=True)\n",
    "        metrics=group_df.iloc[0,:][[\"semantic_WER\",\"semantic_2gram_bleu\",\"semantic_BERT_F1\"]].values\n",
    "        \n",
    "        semantic_WER+=[metrics[0]]\n",
    "        semantic_2gram_bleu+=[metrics[1]]\n",
    "        semantic_BERT_F1+=[metrics[2]]\n",
    "    else:\n",
    "        group_df=whisper3_metric_df.groupby([i]).mean(numeric_only=True)\n",
    "        metrics=group_df.iloc[1,:][[\"semantic_WER\",\"semantic_2gram_bleu\",\"semantic_BERT_F1\"]].values\n",
    "        \n",
    "        semantic_WER+=[metrics[0]]\n",
    "        semantic_2gram_bleu+=[metrics[1]]\n",
    "        semantic_BERT_F1+=[metrics[2]]\n",
    "\n",
    "semantic_WER=[round(j, 3) for j in semantic_WER ]\n",
    "semantic_2gram_bleu=[round(j, 3) for j in semantic_2gram_bleu ]\n",
    "semantic_BERT_F1=[round(j, 3) for j in semantic_BERT_F1 ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "speech_categories = (\"Stutter\",\"Fluent\",\"Prolongation\",\"Block\",\"Sound\\nRepetition\",\"Word\\nRepetition\",\"Interjection\")\n",
    "speech_metrics = {\n",
    "    'Semantic WER': tuple(semantic_WER),\n",
    "    #'Semantic_2gram_BLEU': tuple(semantic_2gram_bleu),\n",
    "    #'Semantic_BERT_F1': tuple(semantic_BERT_F1),\n",
    "}\n",
    "\n",
    "x = np.arange(len(speech_categories))  # the label locations\n",
    "width = 0.5 # the width of the bars\n",
    "multiplier = 0\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "for attribute, measurement in speech_metrics.items():\n",
    "    offset = width * multiplier\n",
    "    rects = ax.bar(x + offset, measurement, width, label=attribute,color=\"darkorange\")\n",
    "    ax.bar_label(rects, padding=3,fontsize='large')\n",
    "    multiplier += 1\n",
    "\n",
    "# Add some text for labels, title and custom x-axis tick labels, etc.\n",
    "ax.set_ylabel('Metric Score',fontsize='large')\n",
    "ax.set_title('Speech to Text WER for Semantic\\n Transcriptions of each Stuttering Type',fontsize='x-large')\n",
    "ax.set_xticks(x, speech_categories,fontsize='large',rotation=90)\n",
    "ax.legend(loc='upper left', ncols=1,fontsize='large')\n",
    "ax.set_ylim(0, 0.6)\n",
    "\n",
    "fig.set_figheight(5)\n",
    "fig.set_figwidth(7)\n",
    "\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "speech_categories = (\"Stutter\",\"Fluent\",\"Prolongation\",\"Block\",\"Sound\\nRepetition\",\"Word\\nRepetition\",\"Interjection\")\n",
    "speech_metrics = {\n",
    "    #'Semantic WER': tuple(semantic_WER),\n",
    "    'Semantic BLEU-2': tuple(semantic_2gram_bleu),\n",
    "    #'Semantic_BERT_F1': tuple(semantic_BERT_F1),\n",
    "}\n",
    "\n",
    "x = np.arange(len(speech_categories))  # the label locations\n",
    "width = 0.5  # the width of the bars\n",
    "multiplier = 0\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "for attribute, measurement in speech_metrics.items():\n",
    "    offset = width * multiplier\n",
    "    rects = ax.bar(x + offset, measurement, width, label=attribute,color=\"green\")\n",
    "    ax.bar_label(rects, padding=3,fontsize='large')\n",
    "    multiplier += 1\n",
    "\n",
    "# Add some text for labels, title and custom x-axis tick labels, etc.\n",
    "ax.set_ylabel('Metric Score',fontsize='large')\n",
    "ax.set_title('Speech to Text BLEU-2 for Semantic\\n Transcriptions of each Stuttering Type',fontsize='x-large')\n",
    "ax.set_xticks(x, speech_categories,fontsize='large',rotation=90)\n",
    "ax.legend(loc='upper left', ncols=1,fontsize='large')\n",
    "ax.set_ylim(0, 0.8)\n",
    "\n",
    "fig.set_figheight(5)\n",
    "fig.set_figwidth(7)\n",
    "\n",
    "\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "speech_categories = (\"Stutter\",\"Fluent\",\"Prolongation\",\"Block\",\"Sound\\nRepetition\",\"Word\\nRepetition\",\"Interjection\")\n",
    "speech_metrics = {\n",
    "    #'Semantic WER': tuple(semantic_WER),\n",
    "    #'Semantic_2gram_BLEU': tuple(semantic_2gram_bleu),\n",
    "    'Semantic BERT F1': tuple(semantic_BERT_F1),\n",
    "}\n",
    "\n",
    "x = np.arange(len(speech_categories))  # the label locations\n",
    "width = 0.5  # the width of the bars\n",
    "multiplier = 0\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "for attribute, measurement in speech_metrics.items():\n",
    "    offset = width * multiplier\n",
    "    rects = ax.bar(x + offset, measurement, width, label=attribute,color=\"C0\")\n",
    "    ax.bar_label(rects, padding=3,fontsize='large')\n",
    "    multiplier += 1\n",
    "\n",
    "# Add some text for labels, title and custom x-axis tick labels, etc.\n",
    "ax.set_ylabel('Metric Score',fontsize='large')\n",
    "ax.set_title('Speech to Text BERTScore F1 for Semantic\\n Transcriptions of each Stuttering Type',fontsize='x-large')\n",
    "ax.set_xticks(x, speech_categories,fontsize='large',rotation=90)\n",
    "ax.legend(loc='upper left', ncols=1,fontsize='large')\n",
    "ax.set_ylim(0, 1.05)\n",
    "\n",
    "fig.set_figheight(5)\n",
    "fig.set_figwidth(7)\n",
    "\n",
    "\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Literal Insertions, deletions, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_avg_literal_wer_comp(name: str, metric: str):\n",
    "    if(name==\"YesStutteredWords\"):\n",
    "        filtered_df=whisper3_metric_df[whisper3_metric_df[\"manual_fluent\"]==0]\n",
    "    else:\n",
    "        filtered_df=whisper3_metric_df[whisper3_metric_df[name]==1]\n",
    "\n",
    "    filtered_df=filtered_df.dropna(subset=[metric])\n",
    "    relative_metric_arr=filtered_df[metric]/filtered_df[\"literal_ref\"]\n",
    "    return relative_metric_arr.mean()\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "literal_sub=[]\n",
    "literal_ins=[]\n",
    "literal_del=[]\n",
    "\n",
    "\n",
    "for i in [\"YesStutteredWords\",\"manual_fluent\",\"manual_prolongation\",\"manual_block\",\"manual_soundRep\",\"manual_wordRep\",\"manual_interject\"]:\n",
    "    if(i==\"YesStutteredWords\"):\n",
    "        literal_sub+=[get_avg_literal_wer_comp(\"YesStutteredWords\",\"literal_sub\")]\n",
    "        literal_ins+=[get_avg_literal_wer_comp(\"YesStutteredWords\",\"literal_ins\")]\n",
    "        literal_del+=[get_avg_literal_wer_comp(\"YesStutteredWords\",\"literal_del\")]\n",
    "    else:\n",
    "        literal_sub+=[get_avg_literal_wer_comp(i,\"literal_sub\")]\n",
    "        literal_ins+=[get_avg_literal_wer_comp(i,\"literal_ins\")]\n",
    "        literal_del+=[get_avg_literal_wer_comp(i,\"literal_del\")]\n",
    "\n",
    "literal_sub=[round(j, 3) for j in literal_sub ]\n",
    "literal_ins=[round(j, 3) for j in literal_ins ]\n",
    "literal_del=[round(j, 3) for j in literal_del ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "literal_sub=[]\n",
    "literal_ins=[]\n",
    "literal_del=[]\n",
    "\n",
    "\n",
    "for i in [\"YesStutteredWords\",\"manual_fluent\",\"manual_prolongation\",\"manual_block\",\"manual_soundRep\",\"manual_wordRep\",\"manual_interject\"]:\n",
    "    literal_sub+=[get_avg_literal_wer_comp(i,\"literal_sub\")]\n",
    "    literal_ins+=[get_avg_literal_wer_comp(i,\"literal_ins\")]\n",
    "    literal_del+=[get_avg_literal_wer_comp(i,\"literal_del\")]\n",
    "\n",
    "literal_sub=[round(j, 3) for j in literal_sub ]\n",
    "literal_ins=[round(j, 3) for j in literal_ins ]\n",
    "literal_del=[round(j, 3) for j in literal_del ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "speech_categories = (\"Stutter\",\"Fluent\",\"Prolongation\",\"Block\",\"Sound\\nRepetition\",\"Word\\nRepetition\",\"Interjection\")\n",
    "speech_metrics = {\n",
    "    'Literal Substitutions': tuple(literal_sub),\n",
    "    'Literal Insertions': tuple(literal_ins),\n",
    "    'Literal Deletions': tuple(literal_del),\n",
    "}\n",
    "\n",
    "x = np.arange(len(speech_categories))  # the label locations\n",
    "width = 0.3  # the width of the bars\n",
    "multiplier = 0\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "for attribute, measurement in speech_metrics.items():\n",
    "    offset = width * multiplier\n",
    "    rects = ax.bar(x + offset, measurement, width, label=attribute)\n",
    "    ax.bar_label(rects, padding=3)\n",
    "    multiplier += 1\n",
    "\n",
    "# Add some text for labels, title and custom x-axis tick labels, etc.\n",
    "ax.set_ylabel('Metric Score')\n",
    "ax.set_title('Speech to Text Relative Substitutions, Insertions, and Deletions for Literal Transcriptions of each Stuttering Type',fontsize='x-large')\n",
    "ax.set_xticks(x+width, speech_categories,fontsize='large')\n",
    "ax.legend(loc='upper left', ncols=1,fontsize='x-large')\n",
    "ax.set_ylim(0, 0.55)\n",
    "\n",
    "fig.set_figheight(5)\n",
    "fig.set_figwidth(16)\n",
    "\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Semantic Subs, Insertions, deletions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_avg_semantic_wer_comp(name: str, metric: str):\n",
    "    if(name==\"YesStutteredWords\"):\n",
    "        filtered_df=whisper3_metric_df[whisper3_metric_df[\"manual_fluent\"]==0]\n",
    "    else:\n",
    "        filtered_df=whisper3_metric_df[whisper3_metric_df[name]==1]\n",
    "\n",
    "    filtered_df=filtered_df.dropna(subset=[metric])\n",
    "    relative_metric_arr=filtered_df[metric]/filtered_df[\"semantic_ref\"]\n",
    "    return relative_metric_arr.mean()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "semantic_sub=[]\n",
    "semantic_ins=[]\n",
    "semantic_del=[]\n",
    "\n",
    "\n",
    "for i in [\"YesStutteredWords\",\"manual_fluent\",\"manual_prolongation\",\"manual_block\",\"manual_soundRep\",\"manual_wordRep\",\"manual_interject\"]:\n",
    "    if(i==\"YesStutteredWords\"):\n",
    "        semantic_sub+=[get_avg_semantic_wer_comp(\"YesStutteredWords\",\"semantic_sub\")]\n",
    "        semantic_ins+=[get_avg_semantic_wer_comp(\"YesStutteredWords\",\"semantic_ins\")]\n",
    "        semantic_del+=[get_avg_semantic_wer_comp(\"YesStutteredWords\",\"semantic_del\")]\n",
    "    else:\n",
    "        semantic_sub+=[get_avg_semantic_wer_comp(i,\"semantic_sub\")]\n",
    "        semantic_ins+=[get_avg_semantic_wer_comp(i,\"semantic_ins\")]\n",
    "        semantic_del+=[get_avg_semantic_wer_comp(i,\"semantic_del\")]\n",
    "\n",
    "semantic_sub=[round(j, 3) for j in semantic_sub ]\n",
    "semantic_ins=[round(j, 3) for j in semantic_ins ]\n",
    "semantic_del=[round(j, 3) for j in semantic_del ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "speech_categories = (\"Stutter\",\"Fluent\",\"Prolongation\",\"Block\",\"Sound\\nRepetition\",\"Word\\nRepetition\",\"Interjection\")\n",
    "speech_metrics = {\n",
    "    'Semantic Substitutions': tuple(semantic_sub),\n",
    "    'Semantic Insertions': tuple(semantic_ins),\n",
    "    'Semantic Deletions': tuple(semantic_del),\n",
    "}\n",
    "\n",
    "x = np.arange(len(speech_categories))  # the label locations\n",
    "width = 0.3  # the width of the bars\n",
    "multiplier = 0\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "for attribute, measurement in speech_metrics.items():\n",
    "    offset = width * multiplier\n",
    "    rects = ax.bar(x + offset, measurement, width, label=attribute)\n",
    "    ax.bar_label(rects, padding=3)\n",
    "    multiplier += 1\n",
    "\n",
    "# Add some text for labels, title and custom x-axis tick labels, etc.\n",
    "ax.set_ylabel('Metric Score')\n",
    "ax.set_title('Speech to Text Relative Substitutions, Insertions, and Deletions for Semantic Transcriptions of each Stuttering Type',fontsize='x-large')\n",
    "ax.set_xticks(x+width, speech_categories,fontsize='large')\n",
    "ax.legend(loc='upper left', ncols=1,fontsize='x-large')\n",
    "ax.set_ylim(0, 0.65)\n",
    "\n",
    "fig.set_figheight(5)\n",
    "fig.set_figwidth(16)\n",
    "\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparing Sematnic vs Literal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_nd_array(columnval):\n",
    "    newdf=whisper3_metric_df[whisper3_metric_df[\"manual_fluent\"]!=1]\n",
    "    nparr=newdf[columnval].to_numpy()\n",
    "    nparr=nparr[~np.isnan(nparr)]\n",
    "    return nparr\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "speech_categories = (\"Literal\",\"Semantic\")\n",
    "speech_metrics = {\n",
    "    'Average WER': (np.mean(get_nd_array(\"literal_WER\")),np.mean(get_nd_array(\"semantic_WER\"))),\n",
    "    #'Average 2 Gram BLEU': (np.mean(get_nd_array(\"literal_2gram_bleu\")),np.mean(get_nd_array(\"semantic_2gram_bleu\"))),\n",
    "    #'Average BERT F1': (np.mean(get_nd_array(\"literal_BERT_F1\")),np.mean(get_nd_array(\"semantic_BERT_F1\"))),\n",
    "}\n",
    "\n",
    "x = np.arange(len(speech_categories))  # the label locations\n",
    "width = 0.5  # the width of the bars\n",
    "multiplier = 0\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "for attribute, measurement in speech_metrics.items():\n",
    "    offset = width * multiplier\n",
    "    rects = ax.bar(x, measurement, width, label=attribute,color=\"darkorange\")\n",
    "    ax.bar_label(rects, padding=1,fontsize='x-large',fmt='%.2f')\n",
    "    multiplier += 1\n",
    "\n",
    "# Add some text for labels, title and custom x-axis tick labels, etc.\n",
    "ax.set_ylabel('Metric Score',fontsize=\"x-large\")\n",
    "ax.set_title(\"WER of Semantic vs Literal\\n Transcription Speech to Text\\n for Stuttered Speech\",fontsize='x-large')\n",
    "ax.set_xticks(x, speech_categories,fontsize='x-large')\n",
    "ax.legend(loc='upper left', ncols=3,fontsize='x-large')\n",
    "ax.set_ylim(0, 0.6)\n",
    "\n",
    "fig.set_figheight(6)\n",
    "fig.set_figwidth(4)\n",
    "\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "speech_categories = (\"Literal\",\"Semantic\")\n",
    "speech_metrics = {\n",
    "    #'Average WER': (np.mean(get_nd_array(\"literal_WER\")),np.mean(get_nd_array(\"semantic_WER\"))),\n",
    "    'Average 2 Gram BLEU': (np.mean(get_nd_array(\"literal_2gram_bleu\")),np.mean(get_nd_array(\"semantic_2gram_bleu\"))),\n",
    "    #'Average BERT F1': (np.mean(get_nd_array(\"literal_BERT_F1\")),np.mean(get_nd_array(\"semantic_BERT_F1\"))),\n",
    "}\n",
    "\n",
    "x = np.arange(len(speech_categories))  # the label locations\n",
    "width = 0.6  # the width of the bars\n",
    "multiplier = 0\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "for attribute, measurement in speech_metrics.items():\n",
    "    offset = width * multiplier\n",
    "    rects = ax.bar(x, measurement, width, label=attribute,color=\"green\")\n",
    "    ax.bar_label(rects, padding=1,fontsize='x-large',fmt='%.2f')\n",
    "    multiplier += 1\n",
    "\n",
    "# Add some text for labels, title and custom x-axis tick labels, etc.\n",
    "ax.set_ylabel('Metric Score',fontsize=\"x-large\")\n",
    "ax.set_title(\"BLEU-2 of Semantic vs Literal\\n Transcription Speech to Text\\n for Stuttered Speech\",fontsize='x-large')\n",
    "ax.set_xticks(x, speech_categories,fontsize='x-large')\n",
    "ax.legend(loc='upper left', ncols=3,fontsize='x-large')\n",
    "ax.set_ylim(0, 0.55)\n",
    "\n",
    "fig.set_figheight(6)\n",
    "fig.set_figwidth(4)\n",
    "\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "speech_categories = (\"Literal\",\"Semantic\")\n",
    "speech_metrics = {\n",
    "    #'Average WER': (np.mean(get_nd_array(\"literal_WER\")),np.mean(get_nd_array(\"semantic_WER\"))),\n",
    "    #'Average 2 Gram BLEU': (np.mean(get_nd_array(\"literal_2gram_bleu\")),np.mean(get_nd_array(\"semantic_2gram_bleu\"))),\n",
    "    'Average BERT F1': (np.mean(get_nd_array(\"literal_BERT_F1\")),np.mean(get_nd_array(\"semantic_BERT_F1\"))),\n",
    "}\n",
    "\n",
    "x = np.arange(len(speech_categories))  # the label locations\n",
    "width = 0.6  # the width of the bars\n",
    "multiplier = 0\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "for attribute, measurement in speech_metrics.items():\n",
    "    offset = width * multiplier\n",
    "    rects = ax.bar(x, measurement, width, label=attribute,color=\"C0\")\n",
    "    ax.bar_label(rects, padding=1,fontsize='x-large',fmt='%.2f')\n",
    "    multiplier += 1\n",
    "\n",
    "# Add some text for labels, title and custom x-axis tick labels, etc.\n",
    "ax.set_ylabel('Metric Score',fontsize=\"x-large\")\n",
    "ax.set_title(\"BERT of Semantic vs Literal\\n Transcription Speech to Text\\n for Stuttered Speech\",fontsize='x-large')\n",
    "ax.set_xticks(x, speech_categories,fontsize='x-large')\n",
    "ax.legend(loc='upper left', ncols=3,fontsize='x-large')\n",
    "ax.set_ylim(0, 0.9)\n",
    "\n",
    "fig.set_figheight(6)\n",
    "fig.set_figwidth(4)\n",
    "\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hallucination Graphing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hallucinations were manually labeled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hallu_df=pd.read_csv(\"<path to hallucination df csv>\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(hallu_df.shape[0]):\n",
    "    if(hallu_df.loc[i,\"manual_hallucination\"]==1):\n",
    "        hallu_df.loc[i,\"hallucination_binary\"]=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_avg_hallucinations(name: str, metric: str):\n",
    "    if(name==\"NoStutteredWords\"):\n",
    "        filtered_df=hallu_df[hallu_df[\"stutter_present\"]==0]\n",
    "    else:\n",
    "        filtered_df=hallu_df[hallu_df[name]==1]\n",
    "\n",
    "    filtered_df=filtered_df.dropna(subset=[metric])\n",
    "    relative_metric_arr=filtered_df[metric].astype(float)\n",
    "    return relative_metric_arr.mean()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "hallucination_binary=[]\n",
    "\n",
    "\n",
    "for i in [\"stutter_present\",\"NoStutteredWords\",\"manual_prolongation\",\"manual_block\",\"manual_soundRep\",\"manual_wordRep\",\"manual_interject\"]:\n",
    "    hallucination_binary+=[get_avg_hallucinations(i,\"hallucination_binary\")]\n",
    "\n",
    "\n",
    "\n",
    "hallucination_binary=[round(j, 3) for j in hallucination_binary ]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "speech_categories = (\"Stutter\",\"Fluent\",\"Prolongation\",\"Block\",\"Sound\\nRepetition\",\"Word\\nRepetition\",\"Interjection\")\n",
    "speech_metrics = {\n",
    "    'Hallucination Frequency': tuple(hallucination_binary),\n",
    "}\n",
    "\n",
    "x = np.arange(len(speech_categories))  # the label locations\n",
    "width = 0.5  # the width of the bars\n",
    "multiplier = 0\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "for attribute, measurement in speech_metrics.items():\n",
    "    offset = width * multiplier\n",
    "    rects = ax.bar(x + offset, measurement, width, label=attribute)\n",
    "    ax.bar_label(rects, padding=1,fontsize=\"large\")\n",
    "    multiplier += 1\n",
    "\n",
    "# Add some text for labels, title and custom x-axis tick labels, etc.\n",
    "ax.set_ylabel('Metric Score')\n",
    "ax.set_title('WhisperV2 Hallucination Frequency',fontsize='xx-large')\n",
    "ax.set_xticks(x, speech_categories,fontsize='large',rotation=90)\n",
    "ax.legend(loc='upper left', ncols=1,fontsize='large')\n",
    "ax.set_ylim(0, 0.25)\n",
    "\n",
    "fig.set_figheight(5)\n",
    "fig.set_figwidth(7)\n",
    "\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aimpower_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
